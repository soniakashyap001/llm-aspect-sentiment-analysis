{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import openai\n",
    "\n",
    "# üëá Fetch API key from Colab sidebar secrets\n",
    "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ‚úÖ Set it for OpenAI client\n",
    "openai.api_key = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Key loaded? ‚úÖ\" if openai.api_key else \"Key missing ‚ùå\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from google.colab import userdata\n",
    "\n",
    "# üëá Fetch from Colab secret storage\n",
    "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ‚úÖ Initialize client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def ask_gpt(prompt, temperature=0.7, top_p=0.9, max_tokens=200, model=\"gpt-3.5-turbo\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in aspect-based sentiment analysis.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"The battery life is terrible but the display quality is outstanding.\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Extract all aspects from the following review, and classify each as Positive, Negative, or Neutral.\n",
    "Return output in JSON format like this:\n",
    "[\n",
    "  {{\"aspect\": \"battery\", \"sentiment\": \"negative\"}},\n",
    "  {{\"aspect\": \"screen\", \"sentiment\": \"positive\"}}\n",
    "]\n",
    "\n",
    "Review: \"{review}\"\n",
    "\"\"\"\n",
    "\n",
    "print(ask_gpt(prompt, temperature=0.3, top_p=0.9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"tomaarsen/setfit-absa-semeval-restaurants\")\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset[\"train\"].to_pandas()\n",
    "df[[\"text\", \"span\", \"label\"]].sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sample_df = df.sample(30, random_state=42)  # use a small sample first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(method, sentence, aspect):\n",
    "    if method == \"zero-shot\":\n",
    "        return f\"\"\"\n",
    "Review: \"{sentence}\"\n",
    "\n",
    "What is the sentiment (positive, negative, or neutral) about the aspect: \"{aspect}\"?\n",
    "\n",
    "Only return the sentiment word.\n",
    "\"\"\"\n",
    "\n",
    "    elif method == \"few-shot\":\n",
    "        return f\"\"\"\n",
    "Examples:\n",
    "Review: \"The service was poor.\" ‚Üí Aspect: service ‚Üí Sentiment: negative\n",
    "Review: \"Loved the food.\" ‚Üí Aspect: food ‚Üí Sentiment: positive\n",
    "\n",
    "Now analyze this:\n",
    "Review: \"{sentence}\"\n",
    "Aspect: \"{aspect}\"\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "    elif method == \"cot\":\n",
    "        return f\"\"\"\n",
    "Let's analyze this step-by-step.\n",
    "\n",
    "Review: \"{sentence}\"\n",
    "Aspect: \"{aspect}\"\n",
    "\n",
    "Step 1: Identify how the sentence describes the aspect.\n",
    "Step 2: Determine whether the sentiment is positive, negative, or neutral.\n",
    "Step 3: Return the sentiment.\n",
    "\n",
    "Answer:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=openai.api_key)  # assuming you've set api_key earlier\n",
    "\n",
    "def ask_gpt(prompt, temperature=0.3, top_p=0.9):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response.choices[0].message.content.strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample 10 reviews from your labeled dataset\n",
    "sample_df = df.sample(10, random_state=42)\n",
    "\n",
    "# Prompting function\n",
    "def get_prompt(method, sentence, aspect):\n",
    "    return f\"\"\"\n",
    "Review: \"{sentence}\"\n",
    "\n",
    "What is the sentiment (positive, negative, or neutral) about the aspect: \"{aspect}\"?\n",
    "\n",
    "Only return the sentiment word.\n",
    "\"\"\"\n",
    "\n",
    "# Ask GPT\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "\n",
    "def ask_gpt(prompt, temperature=0.3, top_p=0.9):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response.choices[0].message.content.strip().lower()\n",
    "\n",
    "# Run small experiment\n",
    "results = []\n",
    "for _, row in sample_df.iterrows():\n",
    "    prompt = get_prompt(\"zero-shot\", row[\"text\"], row[\"span\"])\n",
    "    pred = ask_gpt(prompt, temperature=0.3, top_p=0.9)\n",
    "    gold = row[\"label\"].lower()\n",
    "    match = pred.startswith(gold)\n",
    "\n",
    "    results.append({\n",
    "        \"text\": row[\"text\"],\n",
    "        \"aspect\": row[\"span\"],\n",
    "        \"gold\": gold,\n",
    "        \"predicted\": pred,\n",
    "        \"match\": match\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = df_results[\"match\"].mean()\n",
    "print(f\"‚úÖ Zero-shot Accuracy (10 samples): {accuracy:.2%}\")\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(method, sentence, aspect):\n",
    "    return f\"\"\"\n",
    "Examples:\n",
    "Review: \"The service was poor.\" ‚Üí Aspect: service ‚Üí Sentiment: negative\n",
    "Review: \"Loved the food.\" ‚Üí Aspect: food ‚Üí Sentiment: positive\n",
    "Review: \"The ambiance was okay.\" ‚Üí Aspect: ambiance ‚Üí Sentiment: neutral\n",
    "\n",
    "Now analyze this:\n",
    "Review: \"{sentence}\"\n",
    "Aspect: \"{aspect}\"\n",
    "Sentiment:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for _, row in sample_df.iterrows():\n",
    "    prompt = get_prompt(\"few-shot\", row[\"text\"], row[\"span\"])\n",
    "    pred = ask_gpt(prompt, temperature=0.3, top_p=0.9)\n",
    "    gold = row[\"label\"].lower()\n",
    "    match = pred.startswith(gold)\n",
    "\n",
    "    results.append({\n",
    "        \"text\": row[\"text\"],\n",
    "        \"aspect\": row[\"span\"],\n",
    "        \"gold\": gold,\n",
    "        \"predicted\": pred,\n",
    "        \"match\": match\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = df_results[\"match\"].mean()\n",
    "print(f\"‚úÖ Few-shot Accuracy (10 samples): {accuracy:.2%}\")\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(method, sentence, aspect):\n",
    "    return f\"\"\"\n",
    "Let's analyze this step-by-step.\n",
    "\n",
    "Review: \"{sentence}\"\n",
    "Aspect: \"{aspect}\"\n",
    "\n",
    "Step 1: Identify how the sentence describes the aspect.\n",
    "Step 2: Determine whether the sentiment is positive, negative, or neutral.\n",
    "Step 3: Return only the sentiment word (no explanation).\n",
    "\n",
    "Answer:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, row in sample_df.iterrows():\n",
    "    prompt = get_prompt(\"cot\", row[\"text\"], row[\"span\"])\n",
    "    pred = ask_gpt(prompt, temperature=0.3, top_p=0.9)\n",
    "    gold = row[\"label\"].lower()\n",
    "    match = pred.startswith(gold)\n",
    "\n",
    "    results.append({\n",
    "        \"sample\": i + 1,\n",
    "        \"review\": row[\"text\"],\n",
    "        \"aspect\": row[\"span\"],\n",
    "        \"gold\": gold,\n",
    "        \"gpt_response\": pred,\n",
    "        \"match\": match\n",
    "    })\n",
    "\n",
    "    print(f\"\\n=== Sample {i+1} ===\")\n",
    "    print(f\"Review: {row['text']}\")\n",
    "    print(f\"Aspect: {row['span']}\")\n",
    "    print(f\"Gold: {gold}\")\n",
    "    print(f\"--- GPT Response ---\\n{pred}\")\n",
    "    print(f\"‚úÖ Match: {match}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = df_results[\"match\"].mean()\n",
    "print(f\"‚úÖ CoT Accuracy (10 samples): {accuracy:.2%}\")\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(method, sentence, aspect):\n",
    "    return f\"\"\"\n",
    "You are a sentiment analysis system. Your task is to classify the sentiment of a specific aspect mentioned in a customer review.\n",
    "\n",
    "Instructions:\n",
    "- Determine the sentiment polarity (positive, negative, or neutral) toward the given aspect.\n",
    "- Only return the sentiment word (e.g., positive).\n",
    "\n",
    "Review: \"{sentence}\"\n",
    "Aspect: \"{aspect}\"\n",
    "Sentiment:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(method, sentence, aspect):\n",
    "    return f\"\"\"\n",
    "You are a sentiment analysis system. Your task is to classify the sentiment of a specific aspect mentioned in a customer review.\n",
    "\n",
    "Instructions:\n",
    "- Determine the sentiment polarity (positive, negative, or neutral) toward the given aspect.\n",
    "- Only return the sentiment word (e.g., positive).\n",
    "\n",
    "Review: \"{sentence}\"\n",
    "Aspect: \"{aspect}\"\n",
    "Sentiment:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, row in sample_df.iterrows():\n",
    "    prompt = get_prompt(\"instruction\", row[\"text\"], row[\"span\"])\n",
    "    pred = ask_gpt(prompt, temperature=0.3, top_p=0.9)\n",
    "    gold = row[\"label\"].lower()\n",
    "    match = pred.startswith(gold)\n",
    "\n",
    "    results.append({\n",
    "        \"prompt_type\": \"instruction\",\n",
    "        \"review\": row[\"text\"],\n",
    "        \"aspect\": row[\"span\"],\n",
    "        \"gold\": gold,\n",
    "        \"predicted\": pred,\n",
    "        \"match\": match\n",
    "    })\n",
    "\n",
    "    print(f\"\\n=== Instruction Prompt Sample {i+1} ===\")\n",
    "    print(f\"Review: {row['text']}\")\n",
    "    print(f\"Aspect: {row['span']}\")\n",
    "    print(f\"Gold: {gold}\")\n",
    "    print(f\"GPT: {pred}\")\n",
    "    print(f\"‚úÖ Match: {match}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sum(r[\"match\"] for r in results) / len(results)\n",
    "print(f\"\\n‚úÖ Instruction Prompt Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(method, sentence, aspect):\n",
    "    return f\"\"\"\n",
    "You are an expert aspect-based sentiment analysis reviewer working for a Fortune 500 AI company.\n",
    "\n",
    "Your job is to determine whether the sentiment expressed about a given aspect in a customer review is positive, negative, or neutral.\n",
    "\n",
    "Review: \"{sentence}\"\n",
    "Aspect: \"{aspect}\"\n",
    "Answer only with the sentiment word:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sum(r[\"match\"] for r in results) / len(results)\n",
    "print(f\"\\n‚úÖ Instruction Prompt Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(method, sentence, aspect):\n",
    "    return f\"\"\"\n",
    "Here are some examples:\n",
    "\n",
    "Review: \"The food was great.\"\n",
    "Aspect: food\n",
    "Sentiment: positive\n",
    "\n",
    "Review: \"The staff was rude.\"\n",
    "Aspect: staff\n",
    "Sentiment: negative\n",
    "\n",
    "Review: \"The decor was fine.\"\n",
    "Aspect: decor\n",
    "Sentiment: neutral\n",
    "\n",
    "Now analyze this review:\n",
    "\n",
    "Review: \"{sentence}\"\n",
    "Aspect: {aspect}\n",
    "Sentiment:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "temperatures = [0.0, 0.3, 0.7]\n",
    "top_ps = [0.8, 0.9, 1.0]\n",
    "\n",
    "results = []\n",
    "\n",
    "for temp, top_p in itertools.product(temperatures, top_ps):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    print(f\"\\nüîç Testing Few-shot | Temp={temp}, Top_p={top_p}\")\n",
    "\n",
    "    for _, row in sample_df.iterrows():\n",
    "        prompt = get_prompt(\"few-shot\", row[\"text\"], row[\"span\"])\n",
    "        pred = ask_gpt(prompt, temperature=temp, top_p=top_p)\n",
    "        gold = row[\"label\"].lower()\n",
    "        match = pred.strip().startswith(gold)\n",
    "        correct += match\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"‚úÖ Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    results.append({\n",
    "        \"temperature\": temp,\n",
    "        \"top_p\": top_p,\n",
    "        \"accuracy\": accuracy\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_temp_results = pd.DataFrame(results)\n",
    "df_temp_results.sort_values(by=\"accuracy\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.7\n",
    "top_p = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for _, row in sample_df.iterrows():\n",
    "    prompt = get_prompt(\"few-shot\", row[\"text\"], row[\"span\"])\n",
    "    pred = ask_gpt(prompt, temperature=0.7, top_p=1.0)\n",
    "    gold = row[\"label\"].lower()\n",
    "    match = pred.strip().startswith(gold)\n",
    "    correct += match\n",
    "    total += 1\n",
    "\n",
    "print(f\"\\n‚úÖ Accuracy with temp=0.7 and top_p=1.0: {correct/total:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflect_and_correct(review, aspect, gpt_answer, gold_label):\n",
    "    reflection_prompt = f\"\"\"\n",
    "You are an expert in sentiment analysis.\n",
    "\n",
    "Review: \"{review}\"\n",
    "Aspect: \"{aspect}\"\n",
    "Initial sentiment prediction: \"{gpt_answer}\"\n",
    "Actual correct sentiment: \"{gold_label}\"\n",
    "\n",
    "Was the initial sentiment correct? Answer Yes or No.\n",
    "If No, correct the sentiment and explain why.\n",
    "\"\"\"\n",
    "    return ask_gpt(reflection_prompt, temperature=0.3, top_p=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_reflection = []\n",
    "\n",
    "for i, row in sample_df.iterrows():\n",
    "    prompt = get_prompt(\"few-shot\", row[\"text\"], row[\"span\"])\n",
    "    pred = ask_gpt(prompt, temperature=0.7, top_p=1.0).strip().lower()\n",
    "    gold = row[\"label\"].lower()\n",
    "    match_before = pred.startswith(gold)\n",
    "\n",
    "    corrected_pred = pred  # default to original\n",
    "    reflection_output = \"\"\n",
    "\n",
    "    if not match_before:\n",
    "        reflection_output = reflect_and_correct(row[\"text\"], row[\"span\"], pred, gold)\n",
    "\n",
    "        # Try to extract corrected label from the reflection output\n",
    "        for sentiment in [\"positive\", \"negative\", \"neutral\"]:\n",
    "            if sentiment in reflection_output.lower():\n",
    "                corrected_pred = sentiment\n",
    "                break\n",
    "\n",
    "    match_after = corrected_pred.startswith(gold)\n",
    "\n",
    "    results_reflection.append({\n",
    "        \"text\": row[\"text\"],\n",
    "        \"aspect\": row[\"span\"],\n",
    "        \"gold\": gold,\n",
    "        \"original_pred\": pred,\n",
    "        \"corrected_pred\": corrected_pred,\n",
    "        \"reflection_output\": reflection_output,\n",
    "        \"match_before\": match_before,\n",
    "        \"match_after\": match_after\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reflection = pd.DataFrame(results_reflection)\n",
    "\n",
    "# Before and After Accuracy\n",
    "acc_before = df_reflection[\"match_before\"].mean()\n",
    "acc_after = df_reflection[\"match_after\"].mean()\n",
    "\n",
    "print(f\"\\nüéØ Accuracy BEFORE reflection: {acc_before:.2%}\")\n",
    "print(f\"ü™û Accuracy AFTER reflection:  {acc_after:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar([\"Before Reflection\", \"After Reflection\"], [acc_before, acc_after], color=[\"gray\", \"green\"])\n",
    "plt.title(\"Accuracy Before vs After Reflection\")\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
